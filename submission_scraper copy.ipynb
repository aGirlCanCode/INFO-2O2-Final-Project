{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "AJ1D41VE-WnE"
      },
      "outputs": [],
      "source": [
        "import praw\n",
        "\n",
        "import csv\n",
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "import datetime as dt\n",
        "import pickle\n",
        "\n",
        "import re\n",
        "import string\n",
        "\n",
        "import nltk\n",
        "from nltk import word_tokenize, pos_tag\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "TSH3zIt5P4kc"
      },
      "outputs": [],
      "source": [
        "reddit = praw.Reddit(client_id='0yClWPotAY79LjR16mcelQ',\n",
        "                     client_secret='pTZzcYC9LfrI5YLe61qw6-5KaZLdtQ',\n",
        "                     password='blahblah@123',\n",
        "                     user_agent='Get Reddit data 1.0 by /u/coffeeandcode1111',\n",
        "                     username='coffeeandcode1111')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def setup(id, secret, password, user_agent, username):\n",
        "    reddit = praw.Reddit(client_id = id,\n",
        "                     client_secret = secret,\n",
        "                     password = password,\n",
        "                     user_agent = user_agent,\n",
        "                     username = username)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_reddit_data(subreddit_name, max_count):\n",
        "    \"\"\"Scrapes Reddit submissions and comments.\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    subreddit_name : string\n",
        "        The subreddit name.\n",
        "    max_count : int\n",
        "        The maximum number of posts to query.\n",
        "    \"\"\"\n",
        "    filename = subreddit_name + '_' + str(max_count) + '_' + datetime.now().strftime('%Y%m%d') + '.csv'\n",
        "    # Setting up a csv writer and write the first row \n",
        "    writer = csv.writer(open(filename, 'wt', encoding = 'utf-8'))\n",
        "    writer.writerow(['idstr', 'created', 'created_datetime', 'nsfw', 'flair_text', 'flair_css_class',\n",
        "                     'author', 'title', 'selftext', 'score', 'upvote_ratio', \n",
        "                     'distinguished', 'textlen', 'num_comments', 'top_comments'])   \n",
        "    item_count = 0\n",
        "    comment_count = 0\n",
        "    for submission in reddit.subreddit(subreddit_name).hot(limit=None): \n",
        "        try:\n",
        "            item_count += 1\n",
        "            idstr = submission.id\n",
        "            created = submission.created\n",
        "            created_datetime = datetime.fromtimestamp(created).strftime('%Y' + '-' + '%m' + '-' + '%d')\n",
        "            nsfw = submission.over_18\n",
        "            flair_text = submission.link_flair_text\n",
        "            flair_css_class = submission.link_flair_css_class\n",
        "            author = submission.author\n",
        "            title = submission.title\n",
        "            selftext = submission.selftext\n",
        "            score = submission.score\n",
        "            upvote_ratio = submission.upvote_ratio\n",
        "            distinguished = submission.distinguished\n",
        "            textlen = len(submission.selftext)\n",
        "            num_comments = submission.num_comments\n",
        "            comment_list = []\n",
        "            submission.comments.replace_more(limit=None)\n",
        "            for comment in submission.comments.list():\n",
        "                if comment.author != None:\n",
        "                    comment_count += 1\n",
        "                    comment_list.append(comment.body)\n",
        "            comments = ' '.join(comment_list)\n",
        "            writer.writerow((idstr, created, created_datetime, nsfw, flair_text, flair_css_class,\n",
        "                             author, title, selftext, score, upvote_ratio,\n",
        "                             distinguished, textlen, num_comments, comments))\n",
        "            print('.', end='', flush=True)\n",
        "        except:\n",
        "            print('Error found--resuming...')\n",
        "        if item_count == max_count:\n",
        "            break\n",
        "\n",
        "    if item_count > 0:\n",
        "        print('Done!' + '\\n' + 'Found ' + str(item_count) + ' posts' + \n",
        "              '\\n' + 'Found ' + str(comment_count) + ' comments')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ".....................................................................................................................................................................................................................................................................................................Error found--resuming...\n",
            "........."
          ]
        }
      ],
      "source": [
        "get_reddit_data('Futurology', 1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_reddit_data_1(subreddit_name, max_count):\n",
        "    \"\"\"Scrapes Reddit submissions and comments.\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    subreddit_name : string\n",
        "        The subreddit name.\n",
        "    max_count : int\n",
        "        The maximum number of posts to query.\n",
        "    \"\"\"\n",
        "    filename = 'controversial_'+ subreddit_name + '_' + str(max_count) + '_' + datetime.now().strftime('%Y%m%d') + '.csv'\n",
        "    # Setting up a csv writer and write the first row \n",
        "    writer = csv.writer(open(filename, 'wt', encoding = 'utf-8'))\n",
        "    writer.writerow(['idstr', 'created_datetime', 'flair_text', 'flair_css_class',\n",
        "                     'author', 'title', 'selftext', \n",
        "                     'distinguished', 'textlen'])   \n",
        "    item_count = 0\n",
        "    comment_count = 0\n",
        "    for submission in reddit.subreddit(subreddit_name).controversial(limit=None): \n",
        "        try:\n",
        "            item_count += 1\n",
        "            idstr = submission.id\n",
        "            created = submission.created\n",
        "            created_datetime = datetime.fromtimestamp(created).strftime('%Y' + '-' + '%m' + '-' + '%d')\n",
        "            flair_text = submission.link_flair_text\n",
        "            flair_css_class = submission.link_flair_css_class\n",
        "            author = submission.author\n",
        "            title = submission.title\n",
        "            selftext = submission.selftext\n",
        "            distinguished = submission.distinguished\n",
        "            textlen = len(submission.selftext)\n",
        "            writer.writerow((idstr, created_datetime, flair_text, flair_css_class,\n",
        "                             author, title, selftext,\n",
        "                             distinguished, textlen))\n",
        "            print('.', end='', flush=True)\n",
        "        except:\n",
        "            print('Error found--resuming...')\n",
        "        if item_count == max_count:\n",
        "            break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................."
          ]
        }
      ],
      "source": [
        "get_reddit_data_1('Futurology', 2000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "DATA PREPROCESSING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#NOW - LATEST\n",
        "for key in submissions_info.keys():\n",
        "    filename = '1_submission_id_'+ key + \"_submissions.csv\"\n",
        "    with open(filename, 'r') as f:\n",
        "        df = pd.read_csv(filename)\n",
        "        rem_filename = '1_rem_submission_id_'+ key + \"_submissions.csv\"\n",
        "        mod_filename = '1_mod_submission_id_'+ key + \"_submissions.csv\"\n",
        "        removed_df = df[(df['selftext'] == \"[removed]\") | (df['selftext'] == \"[deleted]\")]\n",
        "        removed_df.to_csv(rem_filename, index=False)\n",
        "        cond1 = df['selftext'].isin(removed_df['selftext'])\n",
        "        print(\"Month\", key, \"Size before exclusion drop\", df.shape)\n",
        "        df.drop(df[cond1].index, inplace = True)\n",
        "        print(\"Month\", key, \"Size before len check\", df.shape)\n",
        "        cond2 = (df['selftext'].str.len() < 30)\n",
        "        df.drop(df[cond2].index, inplace = True)\n",
        "        print(\"Month\", key, \"Size after len check\", df.shape)\n",
        "        df.to_csv(mod_filename, index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "DATA CLEANING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "def pos_filter(text):\n",
        "    '''Given a string of text, tokenize the text and pull out only the nouns and adjectives'''\n",
        "    is_noun_or_adj = lambda pos: pos[:2] in ['NN', 'JJ', 'CD', 'NNP', 'RB', 'VB']\n",
        "    tokenized = word_tokenize(text)\n",
        "    all_nouns_adj = [word for (word, pos) in pos_tag(tokenized) if is_noun_or_adj(pos)] \n",
        "    return ' '.join(all_nouns_adj)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "def clean_text(text):\n",
        "    my_punctuation = string.punctuation\n",
        "    my_punctuation += '’' #frequent occurence in data\n",
        "    my_stopwords = nltk.corpus.stopwords.words('english')\n",
        "    \n",
        "    text_list = []\n",
        "    \n",
        "    text = re.sub('\\w*\\d\\w*', '', text)\n",
        "    text = re.sub('…', '', text)\n",
        "    text = re.sub('['+my_punctuation +']+', ' ', text)\n",
        "\n",
        "    for word in text.split(' '):\n",
        "        if word in my_stopwords or len(word) == 1:\n",
        "            word = \"\"\n",
        "            continue\n",
        "        text_list.append(word)\n",
        "    text = ' '.join(text_list)\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "filename = '1ControversialPostsFuturology.csv'\n",
        "with open(filename, 'r') as f:\n",
        "    df = pd.read_csv(filename)\n",
        "    df['selftext'] = np.where(df['selftext'].isna(), df['title'], df['selftext'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.to_csv('1ControversialPostsFuturology.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "df['selftext'] = df['selftext'].apply(pos_filter)\n",
        "df['selftext'] = df['selftext'].apply(clean_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.to_csv('CleanedControversialPostsFuturology.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'df' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn [1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df\u001b[39m.\u001b[39mhead()\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
          ]
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "TOPIC MODELLING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "comments = df.top_comments.to_list()\n",
        "timestamps = df['created_utc'].apply(lambda x: pd.Timestamp(x))\n",
        "topic_model = BERTopic(n_gram_range=(1,2))\n",
        "topics, _ = topic_model.fit_transform(comments)\n",
        "topics_over_time = topic_model.topics_over_time(docs=comments, \n",
        "                                                timestamps=timestamps, \n",
        "                                                global_tuning=True, \n",
        "                                                evolution_tuning=True, \n",
        "                                                nr_bins=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "topic_model.get_topic_info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "topic_model.get_topics()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "topic_model.visualize_topics_over_time()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "topic_model.visualize_barchart()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "LABELLING EACH TOPIC GROUP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dec_df['Topics'] = dec_df['selftext'].apply(lambda x: dec_topic_model.find_topics(x)[0][0])\n",
        "\n",
        "#save to csv\n",
        "dec_df.to_csv('TM_2021-12-24_submissions.csv')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.8.9 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.9"
    },
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
