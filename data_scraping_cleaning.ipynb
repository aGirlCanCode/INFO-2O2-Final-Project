{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "AJ1D41VE-WnE"
      },
      "outputs": [],
      "source": [
        "import praw\n",
        "\n",
        "import csv\n",
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "import datetime as dt\n",
        "import pickle\n",
        "import numpy as np\n",
        "\n",
        "import re\n",
        "import string\n",
        "\n",
        "import nltk\n",
        "from nltk import word_tokenize, pos_tag\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def setup(id, secret, password, user_agent, username):\n",
        "    \"\"\"Sets up the praw api \"\"\"\n",
        "    reddit = praw.Reddit(client_id = id,\n",
        "                     client_secret = secret,\n",
        "                     password = password,\n",
        "                     user_agent = user_agent,\n",
        "                     username = username)\n",
        "    return reddit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_posts(subreddit, count):\n",
        "    \"\"\"Scrapes Reddit submissions\n",
        "    type subreddit: string, name of the subreddit\n",
        "    type count : int, maximum number of posts to crawl\n",
        "    \"\"\"\n",
        "    filename = 'ControversialPostsFuturology.csv'\n",
        "   \n",
        "    writer = csv.writer(open(filename, 'wt', encoding = 'utf-8'))\n",
        "    #the following attributes were identified and crawled for the each post object\n",
        "    writer.writerow(['idstr', 'created_datetime', 'flair_text', 'flair_css_class',\n",
        "                     'author', 'title', 'selftext', \n",
        "                     'distinguished', 'textlen']) \n",
        "    item_count = 0\n",
        "    comment_count = 0\n",
        "    for submission in reddit.subreddit(subreddit).controversial(limit=None): \n",
        "        try:\n",
        "            item_count += 1\n",
        "            idstr = submission.id\n",
        "            created = submission.created\n",
        "            #convert the utc field into readable datetime format for Dynamic Topic Modelling\n",
        "            created_datetime = datetime.fromtimestamp(created).strftime('%Y' + '-' + '%m' + '-' + '%d')\n",
        "            flair_text = submission.link_flair_text\n",
        "            flair_css_class = submission.link_flair_css_class\n",
        "            author = submission.author\n",
        "            title = submission.title\n",
        "            selftext = submission.selftext\n",
        "            distinguished = submission.distinguished\n",
        "            textlen = len(submission.selftext)\n",
        "            writer.writerow((idstr, created_datetime, flair_text, flair_css_class,\n",
        "                             author, title, selftext,\n",
        "                             distinguished, textlen))\n",
        "            if item_count == count:\n",
        "                break\n",
        "        except:\n",
        "            print(\"Error in fetching the posts\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................."
          ]
        }
      ],
      "source": [
        "get_posts('Futurology', 1000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "DATA CLEANING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "def pos_filter(text):\n",
        "    '''Given a string of text, tokenize the text and pull out only the nouns and adjectives'''\n",
        "    is_noun_or_adj = lambda pos: pos[:2] in ['NN', 'JJ', 'CD', 'NNP', 'RB', 'VB']\n",
        "    tokenized = word_tokenize(text)\n",
        "    all_nouns_adj = [word for (word, pos) in pos_tag(tokenized) if is_noun_or_adj(pos)] \n",
        "    return ' '.join(all_nouns_adj)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "def clean_text(text):\n",
        "    my_punctuation = string.punctuation\n",
        "    my_punctuation += '’' #frequent occurence in data\n",
        "    my_stopwords = nltk.corpus.stopwords.words('english')\n",
        "    \n",
        "    text_list = []\n",
        "    \n",
        "    text = re.sub('\\w*\\d\\w*', '', text)\n",
        "    text = re.sub('…', '', text)\n",
        "    text = re.sub('['+my_punctuation +']+', ' ', text)\n",
        "\n",
        "    for word in text.split(' '):\n",
        "        if word in my_stopwords or len(word) == 1:\n",
        "            word = \"\"\n",
        "            continue\n",
        "        text_list.append(word)\n",
        "    text = ' '.join(text_list)\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "filename = 'ControversialPostsFuturology.csv'\n",
        "#Replace empty selftext field with title field\n",
        "with open(filename, 'r') as f:\n",
        "    df = pd.read_csv(filename)\n",
        "    df['selftext'] = np.where(df['selftext'].isna(), df['title'], df['selftext'])\n",
        "df.to_csv('ControversialPostsFuturology.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Apply data cleaning functions to the attribute of interest - note that pos_tagging has to be done before cleaning because the former takes the letter case and punctuation marks into account for tagging\n",
        "df['selftext'] = df['selftext'].apply(pos_filter)\n",
        "df['selftext'] = df['selftext'].apply(clean_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Save the clean dataframe\n",
        "df.to_csv('CleanedControversialPostsFuturology.csv')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.8.9 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.9 (default, Apr 13 2022, 08:48:06) \n[Clang 13.1.6 (clang-1316.0.21.2.5)]"
    },
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
